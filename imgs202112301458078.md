# Nginx负载均衡分析

在服务器集群中，Nginx起到一个代理服务器的角色（即反向代理），为了避免单独一个服务器压力过大，将来自用户的请求转发给不同的服务器。



```nginx
upstream tomcatservers {
  server 127.0.0.1:8080;
  server 127.0.0.1:8081;
  server 127.0.0.1:8082;
}

# 服务虚拟主机配置
server {
   # 监听端口
   listen       80;
   # 监听服务器ip，域名，或者localhost
   server_name  localhost;
   #access_log  logs/host.access.log  main;

   location / {
       proxy_pass http://tomcatservers;
   }

   error_page   500 502 503 504  /50x.html;
   location = /50x.html {
       root   html;
   }
  
}
```

这就是最基本的负载均衡实例，但这不足以满足实际需求；目前Nginx服务器的upstream模块支持6种方式的分配：

| 轮询               | 默认方式        |
| ------------------ | --------------- |
| weight             | 权重方式        |
| ip_hash            | 依据ip分配方式  |
| least_conn         | 最少连接方式    |
| fair（第三方）     | 响应时间方式    |
| url_hash（第三方） | 依据URL分配方式 |

在这里，只详细说明Nginx自带的负载均衡策略，第三方不多描述。



## 01、轮询

![image-20210823165926395](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457174.png)

最基本的配置方法，上面的例子就是轮询的方式，==它是upstream模块默认的负载均衡默认策略==。每个请求会按时间顺序逐一分配到不同的后端服务器。

有如下参数：

fail_timeout与max_fails结合使用。max_fails设置fail_timeout参数设置的时间内最大失败次数，如果在这个时间内，所有针对该服务器的请求都失败了，那么认为该服务器会被认为是停机了，fail_time服务器会被认为停机的时间长度,默认为10s。backup标记该服务器为备用服务器。当主服务器停止时，请求会被发送到它这里。down标记服务器永久停机了。

注意：

- 在轮询中，如果服务器down掉了，会自动剔除该服务器。
- 缺省配置就是轮询策略。
- 此策略适合服务器配置相当，无状态且短平快的服务使用。



## 02、weight

加权重方式，在轮询策略的基础上指定轮询的几率。例子如下：

![image-20210823165956196](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457175.png)

权重方式，在轮询策略的基础上指定轮询的几率。例子如下：

在该例子中，weight参数用于指定轮询几率，weight的默认值为1,；weight的数值与访问比率成正比，比如Tomcat 7.0被访问的几率为其他服务器的两倍。

官网地址：http://nginx.org/en/docs/stream/ngx_stream_upstream_module.html

weight默认值是：1

注意：

- 权重越高分配到需要处理的请求越多。
- 此策略可以与least_conn和ip_hash结合使用。
- 此策略比较适合服务器的硬件配置差别比较大的情况。

```nginx
upstream tomcatservers {
  server 127.0.0.1:8080 weight=1;
  server 127.0.0.1:8081 weight=2;
  server 127.0.0.1:8082 weight=3;
}
```



## 03、upStream

官网参考：http://nginx.org/en/docs/stream/ngx_stream_upstream_module.html

upstream指令存在一些参数：

- max_conns：

  - 可以限制一台服务器的最大访问连接数。默认值是：0。代表不限制。
  - 其实也就是限流的含义。

  ```nginx
  upstream tomcatservers {
    server 127.0.0.1:8080  max_conns=2;
    server 127.0.0.1:8081  max_conns=2;
    server 127.0.0.1:8082  max_conns=2;
  }
  ```

   

- slow_start

  ```properties
  sets the time during which the server will recover its weight from zero to a nominal value, when unhealthy server becomes healthy, or when the server becomes available after a period of time it was considered unavailable. Default value is zero, i.e. slow start is disabled.
  翻译：当你设置这个值以后，你的权重weight会从0慢慢升级到一个正常的value，可以把一个不健康的服务器编程健康的服务器，它是在一段时间以后在去启动。默认是：0，默认情况下是：关闭的
  ```

  - 可以让一个服务器慢慢的加入到集群中
  - 这个参数不能使用到：hash和random的负载均衡策略中
  - 如果在 upstream 中只有一台 server，则该参数失效

  ```nginx
  upstream tomcatservers {
    server 127.0.0.1:8080  weight=10 slow_start=60s;
    server 127.0.0.1:8081  weight=2;
    server 127.0.0.1:8082  weight=2;
  }
  ```

  校验出现错误

  ``` nginx
  [root@iZuf62zev3la2ivndnxra5Z web1]# nginx -t
  nginx: [emerg] invalid parameter "slow_start=60s" in /usr/local/nginx/nginx/conf/yykk.conf:2
  nginx: configuration file /usr/local/nginx/nginx/conf/nginx.conf test failed
  
  ```

  原因是告诉你：slow_start 只能使用是商业版本nginx中。

- down

  - 作用：用于标识服务器当前的状态。
  - 如果用down进行服务器标记就告诉当前服务器不可用的状态。

  ```nginx
  upstream tomcatservers {
    server 127.0.0.1:8080  weight=10 down;
    server 127.0.0.1:8081  weight=2;
    server 127.0.0.1:8082  weight=2;
  }
  ```

  通过上述的配置以后，8080的服务器就被停止使用了。只能访问到8081和8082。

  

- backup

  - 作用：备机
  - backup 表示当前服务器节点是备用机，只有在其他的服务器都宕机以后，自己才会加入到集群中，被用户访问到
  - 用处：可以用于灰度部署时候的一种更替效果。

  ```nginx
  upstream tomcatservers {
    server 127.0.0.1:8080  weight=10 backup;
    server 127.0.0.1:8081  weight=2;
    server 127.0.0.1:8082  weight=2;
  }
  ```

  - 这个时候正常访问只能访问到8081和8082服务器，8080服务器作为备机
  - 可以尝试吧8081和8082服务挂掉，这个8080服务器生效
  - 在把8081和8082服务启动，这个时候8080又被挂起当做备用机

  

- max_fails 和 fail_timeout
  - 两个参数需要配合一起使用才有意义。max_fails ：表示失败几次，则标记server已宕机，剔出上游服务。fail_timeout ：表示失败的重试时间。
  - max_fails ：最大的失败次数，如果服务器访问的次数超过这个次数就会剔除服务，nginx就会认为这个服务器是挂掉的服务。
  - fail_timeout：失败的时间片段，如果配置fail_timeout=10s,max_fails=5次，代表的含义是：在10s之内如果出现的错误次数大于等于5次的时候，就会认为这个服务器是一个挂掉的服务，nginx会剔除该服务。 
  - max_fails：默认值是：1  
  - fail_timeout的默认值是：10

```nginx
upstream tomcatservers {
  server 127.0.0.1:8080  max_fails=2 fail_timeout=1;
  server 127.0.0.1:8081  weight=1;
  server 127.0.0.1:8082  weight=1;
}
```

则代表在1秒内请求某一server失败达到2次后，则认为该server已经挂了或者宕机了，随后再过1秒，这1秒内不会有新的请求到达刚刚挂掉的节点上，而是会 运作的server，1秒后会再有新请求尝试连接挂掉的server，如果还是失败，重复上一过程，直到恢复。





## 04、ip_hash

 指定负载均衡器按照基于客户端IP的分配方式，这个方法确保了相同的客户端的请求一直发送到相同的服务器，以保证session会话。这样每个访客都固定访问一个后端服务器，可以解决session不能跨服务器的问题。

- 在nginx版本1.3.1之前，不能在ip_hash中使用权重（weight）。

- ip_hash不能与backup同时使用。
- 此策略适合有状态服务，比如session。
- 当有服务器需要剔除，必须手动down掉。

文档：https://nginx.org/en/docs/http/ngx_http_upstream_module.html#ip_hash

![image-20210823170310908](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457176.png)

配置如下：

```nginx
upstream backend {
    ip_hash;

    server backend1.example.com;
    server backend2.example.com;
    server backend3.example.com down;
    server backend4.example.com;
}
```

源码分析

```c

/*
 * Copyright (C) Igor Sysoev
 * Copyright (C) Nginx, Inc.
 */


#include <ngx_config.h>
#include <ngx_core.h>
#include <ngx_http.h>


typedef struct {
    /* the round robin data must be first */
    ngx_http_upstream_rr_peer_data_t   rrp;

    ngx_uint_t                         hash;

    u_char                             addrlen;
    u_char                            *addr;

    u_char                             tries;

    ngx_event_get_peer_pt              get_rr_peer;
} ngx_http_upstream_ip_hash_peer_data_t;


static ngx_int_t ngx_http_upstream_init_ip_hash_peer(ngx_http_request_t *r,
    ngx_http_upstream_srv_conf_t *us);
static ngx_int_t ngx_http_upstream_get_ip_hash_peer(ngx_peer_connection_t *pc,
    void *data);
static char *ngx_http_upstream_ip_hash(ngx_conf_t *cf, ngx_command_t *cmd,
    void *conf);


static ngx_command_t  ngx_http_upstream_ip_hash_commands[] = {

    { ngx_string("ip_hash"),
      NGX_HTTP_UPS_CONF|NGX_CONF_NOARGS,
      ngx_http_upstream_ip_hash,
      0,
      0,
      NULL },

      ngx_null_command
};


static ngx_http_module_t  ngx_http_upstream_ip_hash_module_ctx = {
    NULL,                                  /* preconfiguration */
    NULL,                                  /* postconfiguration */

    NULL,                                  /* create main configuration */
    NULL,                                  /* init main configuration */

    NULL,                                  /* create server configuration */
    NULL,                                  /* merge server configuration */

    NULL,                                  /* create location configuration */
    NULL                                   /* merge location configuration */
};


ngx_module_t  ngx_http_upstream_ip_hash_module = {
    NGX_MODULE_V1,
    &ngx_http_upstream_ip_hash_module_ctx, /* module context */
    ngx_http_upstream_ip_hash_commands,    /* module directives */
    NGX_HTTP_MODULE,                       /* module type */
    NULL,                                  /* init master */
    NULL,                                  /* init module */
    NULL,                                  /* init process */
    NULL,                                  /* init thread */
    NULL,                                  /* exit thread */
    NULL,                                  /* exit process */
    NULL,                                  /* exit master */
    NGX_MODULE_V1_PADDING
};


static u_char ngx_http_upstream_ip_hash_pseudo_addr[3];


static ngx_int_t
ngx_http_upstream_init_ip_hash(ngx_conf_t *cf, ngx_http_upstream_srv_conf_t *us)
{
    if (ngx_http_upstream_init_round_robin(cf, us) != NGX_OK) {
        return NGX_ERROR;
    }

    us->peer.init = ngx_http_upstream_init_ip_hash_peer;

    return NGX_OK;
}


static ngx_int_t
ngx_http_upstream_init_ip_hash_peer(ngx_http_request_t *r,
    ngx_http_upstream_srv_conf_t *us)
{
    struct sockaddr_in                     *sin;
#if (NGX_HAVE_INET6)
    struct sockaddr_in6                    *sin6;
#endif
    ngx_http_upstream_ip_hash_peer_data_t  *iphp;

    iphp = ngx_palloc(r->pool, sizeof(ngx_http_upstream_ip_hash_peer_data_t));
    if (iphp == NULL) {
        return NGX_ERROR;
    }

    r->upstream->peer.data = &iphp->rrp;

    if (ngx_http_upstream_init_round_robin_peer(r, us) != NGX_OK) {
        return NGX_ERROR;
    }

    r->upstream->peer.get = ngx_http_upstream_get_ip_hash_peer;

    switch (r->connection->sockaddr->sa_family) {

    case AF_INET:
        sin = (struct sockaddr_in *) r->connection->sockaddr;
        iphp->addr = (u_char *) &sin->sin_addr.s_addr;
        iphp->addrlen = 3;
        break;

#if (NGX_HAVE_INET6)
    case AF_INET6:
        sin6 = (struct sockaddr_in6 *) r->connection->sockaddr;
        iphp->addr = (u_char *) &sin6->sin6_addr.s6_addr;
        iphp->addrlen = 16;
        break;
#endif

    default:
        iphp->addr = ngx_http_upstream_ip_hash_pseudo_addr;
        iphp->addrlen = 3;
    }

    iphp->hash = 89;
    iphp->tries = 0;
    iphp->get_rr_peer = ngx_http_upstream_get_round_robin_peer;

    return NGX_OK;
}


static ngx_int_t
ngx_http_upstream_get_ip_hash_peer(ngx_peer_connection_t *pc, void *data)
{
    ngx_http_upstream_ip_hash_peer_data_t  *iphp = data;

    time_t                        now;
    ngx_int_t                     w;
    uintptr_t                     m;
    ngx_uint_t                    i, n, p, hash;
    ngx_http_upstream_rr_peer_t  *peer;

    ngx_log_debug1(NGX_LOG_DEBUG_HTTP, pc->log, 0,
                   "get ip hash peer, try: %ui", pc->tries);

    /* TODO: cached */

    ngx_http_upstream_rr_peers_rlock(iphp->rrp.peers);

    if (iphp->tries > 20 || iphp->rrp.peers->single) {
        ngx_http_upstream_rr_peers_unlock(iphp->rrp.peers);
        return iphp->get_rr_peer(pc, &iphp->rrp);
    }

    now = ngx_time();

    pc->cached = 0;
    pc->connection = NULL;

    hash = iphp->hash;

    for ( ;; ) {

        for (i = 0; i < (ngx_uint_t) iphp->addrlen; i++) {
            hash = (hash * 113 + iphp->addr[i]) % 6271;
        }

        w = hash % iphp->rrp.peers->total_weight;
        peer = iphp->rrp.peers->peer;
        p = 0;

        while (w >= peer->weight) {
            w -= peer->weight;
            peer = peer->next;
            p++;
        }

        n = p / (8 * sizeof(uintptr_t));
        m = (uintptr_t) 1 << p % (8 * sizeof(uintptr_t));

        if (iphp->rrp.tried[n] & m) {
            goto next;
        }

        ngx_log_debug2(NGX_LOG_DEBUG_HTTP, pc->log, 0,
                       "get ip hash peer, hash: %ui %04XL", p, (uint64_t) m);

        ngx_http_upstream_rr_peer_lock(iphp->rrp.peers, peer);

        if (peer->down) {
            ngx_http_upstream_rr_peer_unlock(iphp->rrp.peers, peer);
            goto next;
        }

        if (peer->max_fails
            && peer->fails >= peer->max_fails
            && now - peer->checked <= peer->fail_timeout)
        {
            ngx_http_upstream_rr_peer_unlock(iphp->rrp.peers, peer);
            goto next;
        }

        if (peer->max_conns && peer->conns >= peer->max_conns) {
            ngx_http_upstream_rr_peer_unlock(iphp->rrp.peers, peer);
            goto next;
        }

        break;

    next:

        if (++iphp->tries > 20) {
            ngx_http_upstream_rr_peers_unlock(iphp->rrp.peers);
            return iphp->get_rr_peer(pc, &iphp->rrp);
        }
    }

    iphp->rrp.current = peer;

    pc->sockaddr = peer->sockaddr;
    pc->socklen = peer->socklen;
    pc->name = &peer->name;

    peer->conns++;

    if (now - peer->checked > peer->fail_timeout) {
        peer->checked = now;
    }

    ngx_http_upstream_rr_peer_unlock(iphp->rrp.peers, peer);
    ngx_http_upstream_rr_peers_unlock(iphp->rrp.peers);

    iphp->rrp.tried[n] |= m;
    iphp->hash = hash;

    return NGX_OK;
}


static char *
ngx_http_upstream_ip_hash(ngx_conf_t *cf, ngx_command_t *cmd, void *conf)
{
    ngx_http_upstream_srv_conf_t  *uscf;

    uscf = ngx_http_conf_get_module_srv_conf(cf, ngx_http_upstream_module);

    if (uscf->peer.init_upstream) {
        ngx_conf_log_error(NGX_LOG_WARN, cf, 0,
                           "load balancing method redefined");
    }

    uscf->peer.init_upstream = ngx_http_upstream_init_ip_hash;

    uscf->flags = NGX_HTTP_UPSTREAM_CREATE
                  |NGX_HTTP_UPSTREAM_WEIGHT
                  |NGX_HTTP_UPSTREAM_MAX_CONNS
                  |NGX_HTTP_UPSTREAM_MAX_FAILS
                  |NGX_HTTP_UPSTREAM_FAIL_TIMEOUT
                  |NGX_HTTP_UPSTREAM_DOWN;

    return NGX_CONF_OK;
}

```

图解说明：

![image-20210823170519813](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457177.png)

如果是在同一局域网内，算出的hash是一致的，这样他们就会出现相同的hash。







## 05、一致性hash （ip）

一致性hash  ： nginx  / redis 集群/es内部节点

ip_hash存在的问题如下：

- 上面的hash负载均衡存在一个问题，就是如果一个服务器挂掉了，hash的计算就需要全部重新计算，如果是在并发非常大的情况下，用这种算法来实现的话，可能会造成大量的请求在某一个时刻失败。
- 或者节点增加也会发生这种问题的出现
- hash算法还会造成数据的服务的倾斜

解决上面的问题可以使用：一致性hash算法。

一直性Hash算法在很多场景下都有应用，尤其是在分布式缓存系统中，经常用其来进行缓存的访问的负载均衡，比如：redis等<k,v>非关系数据库作为缓存系统。我们首先来看一下采用取模方式进行缓存的问题。

**一致性Hash算法的使用场景**

  假设我们的将10台redis部署为我们的缓存系统，存储<k,v>数据，存储方式是：hash(k)%10，用来将数据分散到各个redis存储系统中。这样做，最大的问题就在于：如果此缓存系统扩展（比如：增加或减少redis服务器的数量），节点故障宕机等将会带来很高的代价。比如：我们业务量增大了，需要扩展我们的缓存系统，再增加一台redis作为缓存服务器，那么后来的数据<k,v>的散列方式变为了：hash(k)%11。我们可以看到，如果我们要查找扩展之前的数据，利用hash(k)%11，则会找不到对应的存储服务器。所以这个时候大量的数据失效了（访问不到了）。
这时候，我们就要进行数据的重现散列，如果是将redis作为存储系统，则需要进行数据迁移，然后进行恢复，但是这个时候就意味着每次增减服务器的时候，集群就需要大量的通信，进行数据迁移，这个开销是非常大的。如果只是缓存，那么缓存就都失效了。这会形成缓存击穿，导致数据库压力巨大，可能会导致应用的崩溃。



 因为对于hash(k)的范围在int范围，所以我们将0~2^32作为一个环。其步骤为：
1，求出每个服务器的hash（服务器ip）值，将其配置到一个 0~2^n 的圆环上（n通常取32）。
2，用同样的方法求出待存储对象的主键 hash值，也将其配置到这个圆环上，然后从数据映射到的位置开始顺时针查找，将数据分布到找到的第一个服务器节点上。
其分布如图：

![img](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457178.png)

这是一致性hash算法的基本原理，接下来我们看一下，此算法是如何解决 我们上边 说的 缓存系统的扩展或者节点宕机导致的缓存失效的问题。比如:再加入一个redis节点：

![img](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457179.png)





**雪崩效应**

接下来我们来看一下，当有节点宕机时会有什么问题。如下图：

![img](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457180.png)

 

如上图，当B节点宕机后，原本存储在B节点的k1，k2将会迁移到节点C上，这可能会导致很大的问题。如果B上存储的是热点数据，将数据迁移到C节点上，然后C需要承受B+C的数据，也承受不住，也挂了。。。。然后继续CD都挂了。这就造成了雪崩效应。
上面会造成雪崩效应的原因分析：
如果不存在热点数据的时候，每台机器的承受的压力是M/2(假设每台机器的最高负载能力为M)，原本是不会有问题的，但是，这个时候A服务器由于有热点数据挂了，然后A的数据迁移至B，导致B所需要承受的压力变为M（还不考虑热点数据访问的压力），所以这个失败B是必挂的，然后C至少需要承受1.5M的压力。。。。然后大家一起挂。。。
所以我们通过上面可以看到，之所以会大家一起挂，原因在于如果一台机器挂了，那么它的压力全部被分配到一台机器上，导致雪崩。

怎么解决雪崩问题呢，这时候需要引入虚拟节点来进行解决。
虚拟节点

虚拟节点，我们可以针对每个实际的节点，虚拟出多个虚拟节点，用来映射到圈上的位置，进行存储对应的数据。如下图：

![img](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457181.png)

 

如上图：A节点对应A1，A2，BCD节点同理。这时候，如果A节点挂了，A节点的数据迁移情况是:A1数据会迁移到C2，A2数据迁移到D1。这就相当于A的数据被C和D分担了，这就避免了雪崩效应的发送，而且虚拟节点我们可以自定义设置，使其适用于我们的应用。

存在的问题
⼀致性哈希算法在服务节点太少时，容易因为节点分部不均匀⽽造成数据倾斜问题。例如系统中只有两台服务器，其环分布如下，节点2只能负责⾮常⼩的⼀段，⼤量的客户端请求落在了节点1上，这就是数据（请求）倾斜问题。

为了解决这种数据倾斜问题，⼀致性哈希算法引⼊了虚拟节点机制，即对每⼀个服务节点计算多个哈希，每个计算结果位置都放置⼀个此服务节点，称为虚拟节点。

具体做法可以在服务器ip或主机名的后⾯增加其他字符来实现。⽐如，可以为每台服务器计算三个虚拟节点，于是可以分别计算 “节点1的ip#1”、“节点1的ip#2”、“节点1的ip#3”、“节点2的ip#1”、“节点2的ip#2”、“节点2的ip#3”的哈希值，于是形成六个虚拟节点，当客户端被路由到虚拟节点的时候其实是被路由到该虚拟节点所对应的真实节点。
![img](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457182)

自定义一致性hash算法：

```java
import java.util.SortedMap;
import java.util.TreeMap;
 
public class Main {
 
    public static void main(String[] args) {
 
        // 定义服务器ip
        String[] tomcatServers = {"192.222.1.30","19.16.1.2","192.168.1.13"};
        SortedMap<Integer,String> serverMap = new TreeMap<>();
 
        // 定义针对每个真实服务器虚拟出来⼏个节点
        int virtaulCount = 3;
 
        for (String tomcatServer : tomcatServers) {
            int hash = Math.abs(tomcatServer.hashCode());
            serverMap.put(hash,tomcatServer);
 
            //创建虚拟节点
            for (int i = 0; i < virtaulCount; i++) {
                int virtaulHash = Math.abs((tomcatServer+"a").hashCode());
                serverMap.put(virtaulHash,"由"+tomcatServer+"的虚拟节点处理");
            }
        }
        //定义客户端
        String[] userServers = {"19.11.12.1","10.113.120.79","107.180.13.5"};
        for (String userServer : userServers){
            int userhash = Math.abs(userServer.hashCode());
            //获取到所有key大于用户hash值的map集合
            SortedMap<Integer,String> sortedMap = serverMap.tailMap(userhash);
 
            if(sortedMap.isEmpty()){
                //如果为空，则表示当前用户节点后没有服务器节点，则使用圆环中的第一个节点
                Integer firstKey = serverMap.firstKey();
                System.out.println("客户端"+userServer+"使用服务器"+serverMap.get(firstKey));
            }else{
                //不为空则寻找第一个大于用户hash的key
                Integer firstKey = sortedMap.firstKey();
                System.out.println("客户端"+userServer+"使用服务器"+serverMap.get(firstKey));
            }
 
        }
    }
}
```

配置一致性hash策略

nginx的负载均衡策略中不包含一致性hash，所以我们需要安装ngx_http_upstream_consistent_hash模块到我们的nginx中

可以到nginx的src/http/modules下查看已经安装的模块，比如ip_hash策略

![img](https://cdn.jsdelivr.net/gh/RayLin24/imgs/imgs202112301457183)

ngx_http_upstream_consistent_hash 模块是⼀个负载均衡器，使⽤⼀个内部⼀致性hash算法来选择合适的后端节点。该模块可以根据配置参数采取不同的⽅式将请求均匀映射到后端机器，
consistent_hash $remote_addr：可以根据客户端ip映射
consistent_hash $request_uri：根据客户端请求的uri映射
consistent_hash $args：根据客户端携带的参数进⾏映

安装步骤：

1）github下载nginx⼀致性hash负载均衡模块 https://github.com/replay/ngx_http_consistent_hash

![img](https://img-blog.csdnimg.cn/20200407112035601.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2xpcXVhbnRvbmc=,size_16,color_FFFFFF,t_70)

2）将下载的压缩包上传到nginx服务器，并解压
3）我们已经编译安装过nginx，此时进⼊当时nginx的安装目录⽬录，执⾏如下命令，等号后边为下载的插件的解压目录

```sh
./configure --add-module=/www/kuangstudy/nginx/ngx_http_consistent_hash-master
```

4）编译和安装

```sh
make && make install
```

5）在nginx.conf⽂件中配置

```
upstream somestream {
  consistent_hash $request_uri;
  server 10.50.1.3:11211;
  server 10.50.1.4:11211;
  server 10.50.1.5:11211;
}
```





## 06、url_hash

- 第三方的负载均衡策略的实现需要安装第三方插件。
- 按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，要配合缓存命中来使用。同一个资源多次请求，可能会到达不同的服务器上，导致不必要的多次下载，缓存命中率不高，以及一些资源时间的浪费。而使用url_hash，可以使得同一个url（也就是同一个资源请求）会到达同一台服务器，一旦缓存住了资源，再此收到请求，就可以从缓存中读取。　

面试题：Nginx负载均衡和SpringCloud（ribbon）负载均衡区别是什么？

```nginx
upstream dynamic_zuoyu {
    hash $request_uri;    #实现每个url定向到同一个后端服务器 
    server localhost:8080;  
    server localhost:8081;  
    server localhost:8082;  
    server localhost:8083;  
}
```

java代码如下：

```java
package com.kuang.contorller;

import com.kuang.utils.ip.IpUtils;
import org.springframework.beans.factory.annotation.Value;
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

import javax.servlet.http.HttpServletRequest;
import java.util.HashMap;
import java.util.Map;

/**
 * @author 飞哥
 * @Title: 学相伴出品
 * @Description: 我们有一个学习网站：https://www.kuangstudy.com
 * @date 2021/8/23 12:31
 */
@RestController
public class TestController {


    @Value("${server.port}")
    private String port;


    @GetMapping("/nginx/test")
    public Map<String, String> test(HttpServletRequest request) {
        Map<String, String> map = new HashMap<>();
        String ipAddr = IpUtils.getIpAddr(request);
        map.put("port1", request.getLocalPort() + "");
        map.put("port2", request.getServerPort() + "");
        map.put("port3", request.getRemotePort() + "");
        map.put("url", request.getRequestURL().toString());
        map.put("port4", port);
        map.put("ipAddr", ipAddr);
        return map;
    }


    @GetMapping("/c/list")
    public Map<String, String> courselist(HttpServletRequest request) {
        Map<String, String> map = new HashMap<>();
        String ipAddr = IpUtils.getIpAddr(request);
        map.put("port1", request.getLocalPort() + "");
        map.put("port2", request.getServerPort() + "");
        map.put("port3", request.getRemotePort() + "");
        map.put("url", request.getRequestURL().toString());
        map.put("port4", port);
        map.put("ipAddr", ipAddr);
        return map;
    }

    @GetMapping("/u/info")
    public Map<String, String> userlist(HttpServletRequest request) {
        Map<String, String> map = new HashMap<>();
        String ipAddr = IpUtils.getIpAddr(request);
        map.put("port1", request.getLocalPort() + "");
        map.put("port2", request.getServerPort() + "");
        map.put("port3", request.getRemotePort() + "");
        map.put("url", request.getRequestURL().toString());
        map.put("port4", port);
        map.put("ipAddr", ipAddr);
        return map;
    }

}

```

访问地址：

http://139.224.164.101/c/list

http://139.224.164.101/u/info

http://139.224.164.101/nginx/test





## 07、least_conn

把请求转发给连接数较少的后端服务器。轮询算法是把请求平均的转发给各个后端，使它们的负载大致相同；但是，有些请求占用的时间很长，会导致其所在的后端负载较高。这种情况下，least_conn这种方式就可以达到更好的负载均衡效果。

配置如下：

```nginx
upstream tomcatservers {

  #把请求转发给连接数较少的后端服务器
  least_conn;
  server 127.0.0.1:8080;
  server 127.0.0.1:8081;
  server 127.0.0.1:8082;
}
```



　注意：

- 此负载均衡策略适合请求处理时间长短不一造成服务器过载的情况。



## 08、fair

按照服务器端的响应时间来分配请求，响应时间短的优先分配。

```nginx
upstream dynamic_zuoyu {
    server 127.0.0.1:8080 fair;
 	server 127.0.0.1:8081;
  	server 127.0.0.1:8082;
}
```





## 08、总结

以上便是6种负载均衡策略的实现方式，其中除了轮询和轮询权重外，都是Nginx根据不同的算法实现的。在实际运用中，需要根据不同的场景选择性运用，大都是多种策略结合使用以达到实际需求。